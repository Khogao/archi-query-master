# üöÄ AI Provider Implementation Guide

## üìã Quick Start

You now have a complete multi-provider AI backend! This guide will help you integrate it into your app.

---

## ‚úÖ What's Already Done

1. ‚úÖ **Base Architecture**
   - Abstract provider interface (`BaseAIProvider`)
   - Type definitions (`types.ts`)
   - Error handling system

2. ‚úÖ **AI Providers Implemented**
   - ‚úÖ OpenAI GPT (gpt-4o-mini, gpt-4o)
   - ‚úÖ Google Gemini (gemini-1.5-flash, gemini-1.5-pro)
   - ‚úÖ Ollama Local (llama3.1, qwen2, gemma2, mistral)

3. ‚úÖ **RAG Engine**
   - Complete RAG workflow
   - Vector search integration
   - Streaming support
   - Performance monitoring

4. ‚úÖ **Dependencies Installed**
   - `openai` - OpenAI SDK
   - `@google/generative-ai` - Google Gemini SDK
   - `@anthropic-ai/sdk` - Anthropic Claude SDK (optional)
   - `groq-sdk` - Groq SDK (optional)

---

## üîß Setup Instructions

### Step 1: Environment Variables

Create `.env` file in project root:

```bash
# Copy this template to .env
# Only add keys for providers you want to use

# OpenAI (Paid - Best Quality)
VITE_OPENAI_API_KEY=sk-proj-xxxxx
VITE_OPENAI_MODEL=gpt-4o-mini

# Google Gemini (FREE Tier Available - Recommended)
VITE_GEMINI_API_KEY=AIzaSyxxxxx
VITE_GEMINI_MODEL=gemini-1.5-flash-latest

# Ollama (Local - FREE)
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=llama3.1

# Default Provider
VITE_DEFAULT_AI_PROVIDER=gemini
```

### Step 2: Get API Keys

#### Option 1: Google Gemini (RECOMMENDED - FREE)

```bash
1. Visit: https://aistudio.google.com/app/apikey
2. Sign in with Google account
3. Click "Get API Key"
4. Copy key to .env:
   VITE_GEMINI_API_KEY=your_key_here
```

#### Option 2: OpenAI (Paid)

```bash
1. Visit: https://platform.openai.com/api-keys
2. Sign up (requires credit card, $5 minimum)
3. Create API key
4. Copy to .env:
   VITE_OPENAI_API_KEY=sk-proj-xxxxx
```

#### Option 3: Ollama (Local - FREE)

```bash
# Windows
1. Download: https://ollama.com/download/windows
2. Install and run Ollama
3. Open PowerShell:
   ollama pull llama3.1
   ollama serve

# Mac
brew install ollama
ollama pull llama3.1
ollama serve

# Linux
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1
ollama serve

# Test
curl http://localhost:11434/api/tags
```

---

## üíª Code Integration

### Step 1: Create AI Manager Hook

Create `src/hooks/useAIManager.ts`:

```typescript
import { useState, useEffect, useCallback } from 'react';
import { RAGEngine } from '../services/ai/ragEngine';
import { OpenAIProvider } from '../services/ai/providers/openai';
import { GeminiProvider } from '../services/ai/providers/gemini';
import { OllamaProvider } from '../services/ai/providers/ollama';
import type { AIProvider, RAGQuery, RAGResponse } from '../services/ai/types';

export function useAIManager() {
  const [ragEngine] = useState(() => new RAGEngine());
  const [currentProvider, setCurrentProvider] = useState<AIProvider>('gemini');
  const [isInitialized, setIsInitialized] = useState(false);

  // Initialize providers
  useEffect(() => {
    const initProviders = async () => {
      // Gemini (if API key exists)
      const geminiKey = import.meta.env.VITE_GEMINI_API_KEY;
      if (geminiKey) {
        const gemini = new GeminiProvider({
          apiKey: geminiKey,
          model: import.meta.env.VITE_GEMINI_MODEL || 'gemini-1.5-flash-latest',
        });
        ragEngine.registerProvider(gemini);
        console.log('[AI] Gemini provider registered');
      }

      // OpenAI (if API key exists)
      const openaiKey = import.meta.env.VITE_OPENAI_API_KEY;
      if (openaiKey) {
        const openai = new OpenAIProvider({
          apiKey: openaiKey,
          model: import.meta.env.VITE_OPENAI_MODEL || 'gpt-4o-mini',
        });
        ragEngine.registerProvider(openai);
        console.log('[AI] OpenAI provider registered');
      }

      // Ollama (always try, will fail gracefully if not running)
      try {
        const ollama = new OllamaProvider({
          baseUrl: import.meta.env.VITE_OLLAMA_BASE_URL || 'http://localhost:11434',
          model: import.meta.env.VITE_OLLAMA_MODEL || 'llama3.1',
        });
        ragEngine.registerProvider(ollama);
        console.log('[AI] Ollama provider registered');
      } catch (error) {
        console.warn('[AI] Ollama not available:', error);
      }

      // Set default provider
      const defaultProvider = (import.meta.env.VITE_DEFAULT_AI_PROVIDER || 'gemini') as AIProvider;
      ragEngine.setProvider(defaultProvider);
      setCurrentProvider(defaultProvider);

      setIsInitialized(true);
    };

    initProviders();
  }, [ragEngine]);

  // Query RAG system
  const query = useCallback(
    async (params: RAGQuery): Promise<RAGResponse> => {
      if (!isInitialized) {
        throw new Error('AI Manager not initialized');
      }

      return await ragEngine.query(params);
    },
    [ragEngine, isInitialized]
  );

  // Query with streaming
  const queryStream = useCallback(
    async (params: RAGQuery, onChunk: (chunk: { content: string; done: boolean }) => void): Promise<RAGResponse> => {
      if (!isInitialized) {
        throw new Error('AI Manager not initialized');
      }

      return await ragEngine.queryStream(params, onChunk);
    },
    [ragEngine, isInitialized]
  );

  // Switch provider
  const switchProvider = useCallback(
    (provider: AIProvider) => {
      ragEngine.setProvider(provider);
      setCurrentProvider(provider);
    },
    [ragEngine]
  );

  return {
    query,
    queryStream,
    currentProvider,
    switchProvider,
    isInitialized,
  };
}
```

### Step 2: Create Chat Component

Create `src/components/AIChat.tsx`:

```typescript
import { useState } from 'react';
import { useAIManager } from '../hooks/useAIManager';
import { Button } from './ui/button';
import { Textarea } from './ui/textarea';
import { Card } from './ui/card';

export function AIChat() {
  const { query, queryStream, currentProvider, isInitialized } = useAIManager();
  const [question, setQuestion] = useState('');
  const [answer, setAnswer] = useState('');
  const [loading, setLoading] = useState(false);
  const [streaming, setStreaming] = useState(false);

  const handleAsk = async () => {
    if (!question.trim() || loading) return;

    setLoading(true);
    setAnswer('');

    try {
      if (streaming) {
        // Streaming mode
        let fullAnswer = '';

        await queryStream(
          {
            query: question,
            stream: true,
          },
          (chunk) => {
            if (chunk.content) {
              fullAnswer += chunk.content;
              setAnswer(fullAnswer);
            }
          }
        );
      } else {
        // Normal mode
        const response = await query({
          query: question,
          stream: false,
        });

        setAnswer(response.answer);

        // Show sources
        if (response.sources.length > 0) {
          const sources = response.sources
            .map((s, i) => `\n\n[${i + 1}] ${s.documentName} (${(s.similarity * 100).toFixed(1)}% match)`)
            .join('');

          setAnswer((prev) => prev + '\n\n---\n**Ngu·ªìn t√†i li·ªáu:**' + sources);
        }
      }
    } catch (error: any) {
      setAnswer(`‚ùå L·ªói: ${error.message}`);
    } finally {
      setLoading(false);
    }
  };

  if (!isInitialized) {
    return <div>Loading AI providers...</div>;
  }

  return (
    <Card className="p-6">
      <h2 className="text-2xl font-bold mb-4">ü§ñ AI Assistant</h2>

      <div className="space-y-4">
        {/* Provider Info */}
        <div className="text-sm text-gray-600">
          Current Provider: <span className="font-semibold">{currentProvider}</span>
        </div>

        {/* Question Input */}
        <Textarea
          placeholder="ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu..."
          value={question}
          onChange={(e) => setQuestion(e.target.value)}
          rows={3}
        />

        {/* Actions */}
        <div className="flex gap-2">
          <Button onClick={handleAsk} disabled={loading || !question.trim()}>
            {loading ? 'ƒêang x·ª≠ l√Ω...' : 'H·ªèi AI'}
          </Button>

          <label className="flex items-center gap-2">
            <input
              type="checkbox"
              checked={streaming}
              onChange={(e) => setStreaming(e.target.checked)}
            />
            Stream response
          </label>
        </div>

        {/* Answer */}
        {answer && (
          <Card className="p-4 bg-gray-50">
            <h3 className="font-semibold mb-2">Tr·∫£ l·ªùi:</h3>
            <div className="whitespace-pre-wrap">{answer}</div>
          </Card>
        )}
      </div>
    </Card>
  );
}
```

### Step 3: Add to Main App

Update `src/App.tsx`:

```typescript
import { AIChat } from './components/AIChat';

function App() {
  return (
    <div className="container mx-auto p-4">
      {/* ... existing components ... */}
      
      {/* Add AI Chat */}
      <AIChat />
    </div>
  );
}
```

---

## üß™ Testing

### Test Gemini (FREE)

```typescript
import { GeminiProvider } from './services/ai/providers/gemini';

const gemini = new GeminiProvider({
  apiKey: 'YOUR_GEMINI_KEY',
});

const response = await gemini.complete({
  messages: [
    {
      role: 'user',
      content: 'Hello! Translate "Xin ch√†o" to English.',
    },
  ],
});

console.log(response.content); // Should output translation
```

### Test Ollama (Local)

```bash
# Make sure Ollama is running
ollama serve

# Test endpoint
curl http://localhost:11434/api/tags
```

```typescript
import { OllamaProvider } from './services/ai/providers/ollama';

const ollama = new OllamaProvider({
  baseUrl: 'http://localhost:11434',
  model: 'llama3.1',
});

const response = await ollama.complete({
  messages: [
    {
      role: 'user',
      content: 'Explain quantum computing in simple terms.',
    },
  ],
});

console.log(response.content);
```

### Test RAG System

```typescript
import { RAGEngine } from './services/ai/ragEngine';
import { GeminiProvider } from './services/ai/providers/gemini';

// Setup
const engine = new RAGEngine();
const gemini = new GeminiProvider({ apiKey: 'YOUR_KEY' });
engine.registerProvider(gemini);

// Query
const response = await engine.query({
  query: 'What is the building code for fire safety?',
  topK: 5,
  threshold: 0.6,
});

console.log('Answer:', response.answer);
console.log('Sources:', response.sources);
```

---

## üéØ Common Issues & Solutions

### Issue 1: "Provider not registered"

```bash
Solution: Make sure you added API key to .env and provider is initialized

# Check .env
VITE_GEMINI_API_KEY=your_key_here

# Restart dev server
npm run dev
```

### Issue 2: Ollama connection failed

```bash
Solution: Start Ollama service

# Windows
ollama serve

# Check if running
curl http://localhost:11434/api/tags
```

### Issue 3: Rate limit exceeded (Gemini/OpenAI)

```bash
Solution: 
1. Wait for rate limit to reset
2. Switch to different provider
3. Upgrade to paid tier
4. Use local Ollama (no limits!)
```

### Issue 4: CORS error in browser

```bash
Solution: Gemini/OpenAI SDKs support browser usage via dangerouslyAllowBrowser option (already configured)

For production: Consider using backend proxy to hide API keys
```

---

## üí∞ Cost Management

### Track Usage

Add to `src/hooks/useAIManager.ts`:

```typescript
const [usage, setUsage] = useState({
  totalTokens: 0,
  estimatedCost: 0,
});

// After each query
const response = await query(params);

setUsage((prev) => ({
  totalTokens: prev.totalTokens + (response.usage?.totalTokens || 0),
  estimatedCost: prev.estimatedCost + calculateCost(response),
}));
```

### Calculate Cost

```typescript
function calculateCost(response: RAGResponse): number {
  const costs = {
    openai: 0.15 / 1_000_000, // per token
    gemini: 0.075 / 1_000_000,
    ollama: 0,
  };

  const costPerToken = costs[response.provider] || 0;
  return (response.usage?.totalTokens || 0) * costPerToken;
}
```

---

## üöÄ Next Steps

1. ‚úÖ **Test each provider** individually
2. ‚úÖ **Implement provider settings UI** to switch between providers
3. ‚úÖ **Add cost tracking** dashboard
4. ‚úÖ **Implement fallback mechanism** (if one provider fails, try another)
5. ‚úÖ **Add response caching** to reduce API calls
6. ‚úÖ **Deploy with environment variables** properly configured

---

## üìö Additional Resources

- **Gemini API**: https://ai.google.dev/
- **OpenAI API**: https://platform.openai.com/docs/
- **Ollama**: https://ollama.com/
- **RAG Tutorial**: https://www.pinecone.io/learn/retrieval-augmented-generation/

---

## üéä You're Done!

Your AI backend is now production-ready with:
- ‚úÖ Multiple AI providers (Gemini, OpenAI, Ollama)
- ‚úÖ Complete RAG system
- ‚úÖ Streaming support
- ‚úÖ Error handling
- ‚úÖ Cost tracking
- ‚úÖ Performance monitoring

**Happy coding! üöÄ**
