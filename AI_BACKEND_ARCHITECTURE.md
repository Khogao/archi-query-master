# ğŸ¤– AI Backend Architecture - RAG System

## ğŸ“‹ Table of Contents
1. [RAG Workflow Overview](#rag-workflow-overview)
2. [AI Provider Options](#ai-provider-options)
3. [Architecture Design](#architecture-design)
4. [Implementation Plan](#implementation-plan)
5. [Cost Analysis](#cost-analysis)
6. [Recommendations](#recommendations)

---

## ğŸ”„ RAG Workflow Overview

### What is RAG (Retrieval-Augmented Generation)?

RAG combines **document retrieval** with **AI generation** to answer questions based on your documents.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG WORKFLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DOCUMENT INGESTION (Offline)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PDF/DOCX â”‚ -> â”‚   OCR    â”‚ -> â”‚ Chunking â”‚ -> â”‚ Embeddingâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚ Vector DBâ”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. QUERY PROCESSING (Real-time)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚User Queryâ”‚ -> â”‚ Embeddingâ”‚ -> â”‚  Search  â”‚ -> â”‚ Top-K    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (Cosine) â”‚    â”‚ Chunks   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. GENERATION (Real-time)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Context  â”‚ -> â”‚   LLM    â”‚ -> â”‚ Response â”‚
   â”‚ + Query  â”‚    â”‚ (AI Model)â”‚    â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current Implementation Status:
- âœ… **Step 1**: Document Ingestion (OCR + Chunking + Embedding)
- âœ… **Step 2**: Query Processing (Embedding + Vector Search)
- âŒ **Step 3**: Generation (Need AI Model) â† **YOU ARE HERE**

---

## ğŸ¤– AI Provider Options

### ğŸ  **Option 1: Local AI (FREE, Privacy-focused)**

#### 1.1. **Ollama** (â­ RECOMMENDED for Local)
```
Pros:
âœ… Completely free
âœ… Privacy - data stays on your machine
âœ… Easy installation & management
âœ… Support nhiá»u models: Llama 3, Mistral, Gemma, Qwen
âœ… API compatible vá»›i OpenAI
âœ… Vietnamese support (Qwen, Llama 3)
âœ… No internet required after download

Cons:
âŒ Requires good GPU/CPU (8GB RAM minimum)
âŒ Slower than cloud APIs
âŒ Limited model size (3B-13B parameters optimal)

Models:
- llama3.1:8b (Best general, Vietnamese OK)
- qwen2:7b (Best Vietnamese)
- gemma2:9b (Fast, good quality)
- mistral:7b (Good reasoning)

Installation:
1. Download from ollama.com
2. Run: ollama pull llama3.1
3. API endpoint: http://localhost:11434
```

#### 1.2. **LM Studio**
```
Pros:
âœ… User-friendly GUI
âœ… Model marketplace
âœ… OpenAI-compatible API

Cons:
âŒ Heavier than Ollama
âŒ Limited model selection
```

#### 1.3. **Hugging Face Transformers.js** (Already Using)
```
Pros:
âœ… Already integrated in project
âœ… Runs in browser (WebGPU)
âœ… Good for embeddings

Cons:
âŒ Limited for text generation (small models only)
âŒ Slow in browser
```

---

### â˜ï¸ **Option 2: Cloud AI (Paid, High-quality)**

#### 2.1. **OpenAI GPT** (â­ BEST Quality)
```
Models:
- GPT-4o: $2.50/1M input, $10/1M output (Best quality)
- GPT-4o-mini: $0.15/1M input, $0.60/1M output (Good balance)
- GPT-3.5-turbo: $0.50/1M input, $1.50/1M output (Cheapest)

Pros:
âœ… Best quality for Vietnamese
âœ… Reliable, fast
âœ… Great documentation
âœ… Streaming support

Cons:
âŒ Most expensive
âŒ Requires API key ($5 minimum)
âŒ Data sent to OpenAI servers

Usage Estimate:
- 1000 queries/month: ~$2-5
- 10,000 queries/month: ~$20-50
```

#### 2.2. **Google Gemini** (â­ BEST Free Tier)
```
Models:
- Gemini 1.5 Flash: FREE (15 RPM, 1M TPM)
- Gemini 1.5 Pro: FREE (2 RPM, 32K TPM)
- Paid: $0.075/1M input, $0.30/1M output

Pros:
âœ… FREE tier generous (60 requests/minute)
âœ… Excellent Vietnamese support
âœ… Fast response
âœ… Long context (2M tokens)
âœ… Good for RAG

Cons:
âŒ Rate limits on free tier
âŒ Newer, less documentation than OpenAI

Usage Estimate:
- Free tier: ~900 queries/month
- Paid: ~$1-3/1000 queries
```

#### 2.3. **Anthropic Claude** (â­ BEST Reasoning)
```
Models:
- Claude 3.5 Sonnet: $3/1M input, $15/1M output
- Claude 3 Haiku: $0.25/1M input, $1.25/1M output

Pros:
âœ… Best reasoning & analysis
âœ… Safety-focused
âœ… Good Vietnamese
âœ… Large context (200K tokens)

Cons:
âŒ No free tier
âŒ More expensive than others
âŒ Waitlist for API access
```

#### 2.4. **Groq** (â­ FASTEST)
```
Models:
- Llama 3.1 70B: FREE (30 RPM)
- Mixtral 8x7B: FREE (30 RPM)

Pros:
âœ… EXTREMELY fast (800 tokens/sec)
âœ… FREE tier very generous
âœ… Open-source models
âœ… Good quality

Cons:
âŒ Vietnamese not as good as GPT/Gemini
âŒ Rate limits
```

---

## ğŸ—ï¸ Architecture Design

### Multi-Provider Architecture

```typescript
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Frontend (React)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            AI Provider Manager                    â”‚  â”‚
â”‚  â”‚  - Provider selection                            â”‚  â”‚
â”‚  â”‚  - API key management                            â”‚  â”‚
â”‚  â”‚  - Fallback logic                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â–¼                â–¼                â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  OpenAI  â”‚    â”‚  Gemini  â”‚    â”‚  Ollama  â”‚         â”‚
â”‚  â”‚ Provider â”‚    â”‚ Provider â”‚    â”‚ Provider â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                â”‚                â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                          â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         RAG Query Engine                         â”‚  â”‚
â”‚  â”‚  1. Embed query                                  â”‚  â”‚
â”‚  â”‚  2. Search vector DB                             â”‚  â”‚
â”‚  â”‚  3. Retrieve top-K chunks                        â”‚  â”‚
â”‚  â”‚  4. Build context prompt                         â”‚  â”‚
â”‚  â”‚  5. Call AI provider                             â”‚  â”‚
â”‚  â”‚  6. Stream response                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure

```
src/
â”œâ”€â”€ services/
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ providers/
â”‚       â”‚   â”œâ”€â”€ base.ts           # Abstract provider interface
â”‚       â”‚   â”œâ”€â”€ openai.ts         # OpenAI implementation
â”‚       â”‚   â”œâ”€â”€ gemini.ts         # Google Gemini implementation
â”‚       â”‚   â”œâ”€â”€ ollama.ts         # Ollama implementation
â”‚       â”‚   â”œâ”€â”€ anthropic.ts      # Claude implementation
â”‚       â”‚   â””â”€â”€ groq.ts           # Groq implementation
â”‚       â”œâ”€â”€ ragEngine.ts          # RAG query engine
â”‚       â”œâ”€â”€ providerManager.ts   # Provider selection & fallback
â”‚       â””â”€â”€ types.ts              # TypeScript types
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useAIChat.ts              # React hook for AI chat
â””â”€â”€ components/
    â””â”€â”€ AIProviderSettings.tsx    # UI for provider selection
```

---

## ğŸ“ Implementation Plan

### Phase 1: Core Infrastructure (Day 1)
```
âœ… Create AI provider interface
âœ… Implement OpenAI provider
âœ… Implement Gemini provider
âœ… Implement Ollama provider
âœ… Create RAG query engine
âœ… Add environment variables
```

### Phase 2: Integration (Day 2)
```
âœ… Create useAIChat hook
âœ… Integrate with existing vector search
âœ… Add provider selection UI
âœ… Add API key management
âœ… Add streaming support
```

### Phase 3: Testing & Polish (Day 3)
```
âœ… Test all providers
âœ… Add error handling
âœ… Add retry logic
âœ… Add cost tracking
âœ… Add response caching
âœ… Add fallback mechanism
```

---

## ğŸ’° Cost Analysis

### Scenario: 1000 queries/month, avg 500 tokens input + 200 tokens output

| Provider | Model | Cost/1000 queries | Free Tier | Speed | Quality |
|----------|-------|------------------|-----------|-------|---------|
| **Ollama** | llama3.1:8b | **$0** | Unlimited | Slow (5-10s) | Good |
| **Gemini** | 1.5 Flash | **$0** (free tier) | 900/month | Fast (1-2s) | Excellent |
| **OpenAI** | GPT-4o-mini | $1.50 | No | Fast (1-2s) | Excellent |
| **OpenAI** | GPT-4o | $15 | No | Medium (2-4s) | Best |
| **Groq** | Llama 3.1 70B | **$0** (free tier) | 900/month | Fastest (0.5s) | Good |
| **Anthropic** | Haiku | $2 | No | Fast (1-2s) | Very Good |

### Cost Projection (1 year, 10K queries/month)

```
Low Usage (1K/month):
- Ollama: $0 âœ…
- Gemini Free: $0 âœ…
- Groq Free: $0 âœ…
- OpenAI GPT-4o-mini: $18/year
- OpenAI GPT-4o: $180/year

Medium Usage (10K/month):
- Ollama: $0 âœ…
- Gemini Paid: $12/year
- Groq: Need paid plan
- OpenAI GPT-4o-mini: $180/year
- OpenAI GPT-4o: $1,800/year

High Usage (100K/month):
- Ollama: $0 âœ… (if hardware sufficient)
- Gemini Paid: $120/year
- OpenAI GPT-4o-mini: $1,800/year
- OpenAI GPT-4o: $18,000/year
```

---

## â­ Recommendations

### **For Development (Learning Phase)**
```
1. Start with: Gemini 1.5 Flash (FREE)
   âœ… No cost
   âœ… Great quality
   âœ… Fast
   âœ… Easy setup

2. Backup: Ollama llama3.1:8b (LOCAL)
   âœ… Privacy
   âœ… Offline capability
   âœ… No API limits
```

### **For Production (Small Scale)**
```
1. Primary: Gemini 1.5 Flash (FREE tier)
2. Fallback: Ollama (LOCAL)
3. Premium: OpenAI GPT-4o-mini (if budget allows)

Strategy:
- Free tier users â†’ Gemini/Ollama
- Paid users â†’ OpenAI GPT-4o-mini
- Enterprise â†’ OpenAI GPT-4o
```

### **For Production (Large Scale)**
```
1. Primary: OpenAI GPT-4o-mini ($180/10K queries)
2. Fallback: Gemini 1.5 Flash (Paid tier)
3. Local: Ollama for offline/privacy-sensitive users

Strategy:
- Load balance between providers
- Cache common queries
- Use cheaper models for simple questions
- Use GPT-4o only for complex reasoning
```

---

## ğŸ”§ Technical Implementation

### Environment Variables (.env)

```bash
# OpenAI
VITE_OPENAI_API_KEY=sk-proj-xxx
VITE_OPENAI_MODEL=gpt-4o-mini

# Google Gemini
VITE_GEMINI_API_KEY=AIzaSyxxx
VITE_GEMINI_MODEL=gemini-1.5-flash-latest

# Anthropic Claude
VITE_ANTHROPIC_API_KEY=sk-ant-xxx
VITE_ANTHROPIC_MODEL=claude-3-haiku-20240307

# Groq
VITE_GROQ_API_KEY=gsk_xxx
VITE_GROQ_MODEL=llama-3.1-70b-versatile

# Ollama (Local)
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=llama3.1

# Default Provider
VITE_DEFAULT_AI_PROVIDER=gemini  # gemini | openai | ollama | anthropic | groq
```

### API Keys - How to Get

```bash
1. OpenAI:
   - Visit: https://platform.openai.com/api-keys
   - Sign up (need credit card)
   - Minimum $5 deposit
   - Get API key

2. Google Gemini:
   - Visit: https://aistudio.google.com/app/apikey
   - Sign in with Google account
   - Click "Get API Key"
   - FREE (no credit card required) âœ…

3. Anthropic:
   - Visit: https://console.anthropic.com/
   - Sign up (waitlist)
   - Request API access

4. Groq:
   - Visit: https://console.groq.com/
   - Sign up (free)
   - Get API key
   - FREE tier available âœ…

5. Ollama:
   - Download: https://ollama.com/download
   - Install locally
   - Run: ollama pull llama3.1
   - No API key needed âœ…
```

---

## ğŸš€ Quick Start Guide

### Step 1: Install Ollama (Local AI)
```bash
# Windows/Mac/Linux
1. Download from https://ollama.com/download
2. Install
3. Open terminal:
   ollama pull llama3.1
   ollama serve

# Test
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "Hello!"
}'
```

### Step 2: Get Gemini API Key (FREE)
```bash
1. Visit https://aistudio.google.com/app/apikey
2. Click "Get API Key"
3. Copy key
4. Add to .env:
   VITE_GEMINI_API_KEY=your_key_here
```

### Step 3: Install Dependencies
```bash
npm install openai @google/generative-ai @anthropic-ai/sdk
```

### Step 4: Run Project
```bash
npm run dev
```

---

## ğŸ¯ Next Steps

I will now implement:

1. âœ… **Base provider interface** - Abstract class for all providers
2. âœ… **OpenAI provider** - GPT-4o-mini implementation
3. âœ… **Gemini provider** - Gemini 1.5 Flash implementation
4. âœ… **Ollama provider** - Local llama3.1 implementation
5. âœ… **RAG engine** - Query processing & context building
6. âœ… **Provider manager** - Selection & fallback logic
7. âœ… **React hook** - useAIChat for easy integration
8. âœ… **UI components** - Settings panel for provider selection

---

## ğŸ“š Additional Resources

- Ollama: https://ollama.com/
- Gemini API: https://ai.google.dev/
- OpenAI API: https://platform.openai.com/docs/
- RAG Tutorial: https://www.pinecone.io/learn/retrieval-augmented-generation/

---

**Ready to implement! ğŸš€**

See `AI_PROVIDER_IMPLEMENTATION.md` for detailed code implementation.
