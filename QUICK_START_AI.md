# üöÄ QUICK START - AI RAG SETUP

## ‚ö° **OPTION 1: GEMINI FREE (2 PH√öT - KHUY·∫æN NGH·ªä)**

### **B∆∞·ªõc 1: L·∫•y API Key (FREE, kh√¥ng c·∫ßn th·∫ª)**
1. M·ªü: https://aistudio.google.com/app/apikey
2. Click **"Create API Key"**
3. Copy API key (d·∫°ng: `AIzaSyxxxxx...`)

### **B∆∞·ªõc 2: Th√™m v√†o .env file**
```bash
# M·ªü file .env v√† thay YOUR_GEMINI_API_KEY_HERE b·∫±ng key v·ª´a l·∫•y
VITE_GEMINI_API_KEY=AIzaSyxxxxx...  # <-- Thay ƒë√¢y
VITE_DEFAULT_AI_PROVIDER=gemini
```

### **B∆∞·ªõc 3: Restart dev server**
```bash
# T·∫Øt terminal hi·ªán t·∫°i (Ctrl+C)
npm run dev
```

### **B∆∞·ªõc 4: Test ngay!**
1. M·ªü browser: http://localhost:8080
2. Upload t√†i li·ªáu (PDF/DOCX) t·ª´ folder:
   ```
   D:\OneDrive\Phi\Project mamagement tools\Thu tuc - Phap ly
   ```
3. Click tab **"AI Chat"**
4. H·ªèi c√¢u h·ªèi v·ªÅ t√†i li·ªáu!

---

## üè† **OPTION 2: OLLAMA LOCAL (10 PH√öT - PRIVATE)**

### **B∆∞·ªõc 1: C√†i Ollama**
1. Download: https://ollama.com/download/windows
2. Ch·∫°y installer
3. M·ªü PowerShell m·ªõi, test:
   ```bash
   ollama --version
   ```

### **B∆∞·ªõc 2: T·∫£i model AI**
```bash
# Ch·ªçn 1 trong c√°c model sau:

# qwen2:7b - KHUY·∫æN NGH·ªä (t·ªët nh·∫•t cho ti·∫øng Vi·ªát)
ollama pull qwen2:7b

# llama3.1:8b - T·ªët cho ti·∫øng Anh
ollama pull llama3.1:8b

# gemma2:9b - C√¢n b·∫±ng
ollama pull gemma2:9b
```

**L∆∞u √Ω:** Model ~4-5GB, t·∫£i m·∫•t ~5-10 ph√∫t

### **B∆∞·ªõc 3: Ch·∫°y Ollama server**
```bash
# M·ªü PowerShell m·ªõi, ch·∫°y:
ollama serve
```

**Gi·ªØ terminal n√†y m·ªü!** Server ch·∫°y ·ªü http://localhost:11434

### **B∆∞·ªõc 4: C·∫•u h√¨nh .env**
```bash
# Uncomment v√† set:
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=qwen2:7b
VITE_DEFAULT_AI_PROVIDER=ollama
```

### **B∆∞·ªõc 5: Restart dev server**
```bash
npm run dev
```

### **B∆∞·ªõc 6: Test!**
Same nh∆∞ Option 1, upload v√† chat!

---

## üéØ **TEST WORKFLOW**

### **1. Upload Documents:**
```
1. Click "Document Management" tab
2. Click "Upload Documents" 
3. Ch·ªçn files t·ª´: D:\OneDrive\Phi\Project mamagement tools\Thu tuc - Phap ly
4. Ch·ªù OCR + processing (5-30s per file)
5. Th·∫•y danh s√°ch documents
```

### **2. Ask Questions:**
```
1. Click "AI Chat" tab
2. Select provider: Gemini ho·∫∑c Ollama
3. Type c√¢u h·ªèi, v√≠ d·ª•:
   - "T√≥m t·∫Øt c√°c th·ªß t·ª•c ph√°p l√Ω trong t√†i li·ªáu"
   - "Gi·∫£i th√≠ch quy tr√¨nh xin ph√©p x√¢y d·ª±ng"
   - "Li·ªát k√™ c√°c gi·∫•y t·ªù c·∫ßn thi·∫øt"
4. Click "Ask" ho·∫∑c b·∫≠t streaming
5. Xem k·∫øt qu·∫£ v·ªõi citations!
```

### **3. Test Vector Search (Traditional):**
```
1. Click "Query Panel" tab
2. Type: "th·ªß t·ª•c ph√°p l√Ω"
3. Select folders
4. Click "Search"
5. Xem top-K relevant chunks
```

---

## üîÑ **SWITCH PROVIDERS**

B·∫°n c√≥ th·ªÉ d√πng nhi·ªÅu providers c√πng l√∫c!

### **Setup c·∫£ 2:**
```bash
# .env
VITE_GEMINI_API_KEY=AIzaSyxxxxx...
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=qwen2:7b
VITE_DEFAULT_AI_PROVIDER=gemini  # M·∫∑c ƒë·ªãnh
```

### **Switch trong UI:**
```
AI Chat tab -> Dropdown "Provider" -> Ch·ªçn Gemini/Ollama
```

---

## üìä **PERFORMANCE COMPARISON**

| Provider | Speed | Quality | Cost | Vietnamese | Privacy |
|----------|-------|---------|------|------------|---------|
| **Gemini** | ‚ö°‚ö°‚ö° (1-2s) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üÜì FREE | ‚úÖ Excellent | ‚ö†Ô∏è Cloud |
| **Ollama qwen2** | ‚ö°‚ö° (3-8s) | ‚≠ê‚≠ê‚≠ê‚≠ê | üÜì FREE | ‚úÖ Very Good | ‚úÖ Local |
| **OpenAI** | ‚ö°‚ö°‚ö° (1-2s) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ Paid | ‚úÖ Best | ‚ö†Ô∏è Cloud |

**Khuy·∫øn ngh·ªã:**
- **Development:** Gemini (nhanh, FREE, d·ªÖ setup)
- **Production (private data):** Ollama (local, secure)
- **Production (best quality):** OpenAI (n·∫øu c√≥ budget)

---

## üêõ **TROUBLESHOOTING**

### **Error: "Provider not initialized"**
```bash
# Check .env c√≥ API key ch∆∞a
# Restart dev server
npm run dev
```

### **Error: "Failed to fetch Ollama"**
```bash
# Check Ollama server ƒëang ch·∫°y:
ollama serve

# Check URL trong .env:
VITE_OLLAMA_BASE_URL=http://localhost:11434
```

### **Error: "Model not found"**
```bash
# List models available:
ollama list

# Pull model:
ollama pull qwen2:7b
```

### **OCR ch·∫≠m:**
```
Normal! PDF scanned m·∫•t 2-5s/page
Scanned documents c·∫ßn Tesseract OCR
Text-based PDFs nhanh h∆°n (<1s/page)
```

### **No results in vector search:**
```
1. Check documents ƒë√£ upload ch∆∞a
2. Check embeddings generated (console.log)
3. Try lower threshold (0.5 instead of 0.7)
4. Try broader query
```

---

## ‚úÖ **CHECKLIST**

- [ ] API key added to .env
- [ ] Dev server restarted
- [ ] Documents uploaded successfully
- [ ] AI Chat tab visible
- [ ] Provider shows "online" (green)
- [ ] Question answered with sources
- [ ] Response makes sense
- [ ] Citations link to correct chunks

---

## üéâ **SUCCESS!**

N·∫øu t·∫•t c·∫£ work, b·∫°n ƒë√£ c√≥:
‚úÖ Production-ready RAG system
‚úÖ Multi-provider AI backend
‚úÖ Real OCR for Vietnamese documents
‚úÖ Persistent storage (offline-first)
‚úÖ Vector search with embeddings
‚úÖ AI chat with streaming

**Next steps:**
- Test v·ªõi nhi·ªÅu documents
- Compare providers
- Deploy to production
- Add more features!

---

**Need help?** Check:
- AI_BACKEND_ARCHITECTURE.md (detailed guide)
- AI_PROVIDER_IMPLEMENTATION.md (setup guide)
- CODEBASE_SCAN_REPORT.md (full overview)
